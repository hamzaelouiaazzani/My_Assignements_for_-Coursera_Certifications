{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Learn Convolutional Neral Network</h5>\n",
    "<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines \n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li><a href=\"#ref0\">Helper functions </a></li>\n",
    "\n",
    "<li><a href=\"#ref1\"> Prepare Data </a></li>\n",
    "<li><a href=\"#ref2\">Convolutional Neral Network </a></li>\n",
    "<li><a href=\"#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n",
    "<li><a href=\"#ref4\">Analyse Results</a></li>\n",
    "\n",
    "<br>\n",
    "<p></p>\n",
    "Estimated Time Needed: <strong>25 min</strong>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "<h2 align=center>Helper functions </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8ebc111c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "<h2 align=center>Prepare Data </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x7f8e70901410>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGxCAYAAADLfglZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZJklEQVR4nO3df2yUhR3H8c9R7NG69hBYWxpaLK5JEQYCZYbfTKSJkm5IwhyKY5IsYyk/arOtIG4oGz3ASUwsP1ISCYaguMwibjNbp67AkFBLqw1uID9GG1lTdOSuoFxt++yPxSO1pVT6HN/r9f1Knj/63MM9Xy/13nme53qPx3EcRwAAGBhgPQAAoP8iQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUJAlDp79qwWLFigwYMH6xvf+Ibmzp2r48ePW48FuIoIAVHo4sWLmjFjhk6dOqUXX3xRr776qq5evarZs2fr5MmT1uMBrvHw3XFA9PnlL3+p559/Xh999JFGjhwpSQoGg7rrrrt03333ad++fcYTAu7gSAi4CYcOHZLH49HLL7/c6bGXXnpJHo9HVVVVN/385eXluu+++8IBkqTk5GQtWLBAb7zxhlpbW2/6uYFoQoSAmzBjxgxNmDBBW7du7fRYaWmpJk+erMmTJ8txHLW2tvZo+dLnn3+uM2fOaNy4cZ2ee9y4cfr888919uzZiP73AbcKEQJu0sqVK/WPf/xDtbW14XVVVVWqqqrS8uXLJUm7d+/Wbbfd1qPlS5cuXZLjOBoyZEinfX657tNPP43sfxxwiwy0HgDoqxYtWqTi4mJt3bpVO3fulCS98MIL+uY3v6mHH35YkpSfn3/Tp+U8Hs9NPQb0JUQIuEler1c//elP9dxzz+nZZ5/VF198oVdffVVFRUXyer2S/n/k4vP5vtbz3nHHHfJ4PF0e7fz3v/8NPy8QCzgdB/TCz372M33xxRd68cUXtXPnTrW2tmrZsmXhx2/mdFxCQoK+9a1vqa6urtP+6urqlJCQoFGjRt2S/z4g0jgSAnph+PDhWrhwobZt26aWlhbl5+crMzMz/PjNno576KGH9Pzzz6uhoUEZGRmSpObmZr322mv63ve+p4ED+V8XsYG/EwJ66dixY7r33nslSX/72980Z86cXj/nxYsXNX78eA0bNkzr16+X1+vVxo0bVVNTo2PHjiknJ6fX+wCiARECXJCVlaWEhAR9+OGHrj3nmTNn9POf/1xvv/22WltbNWXKFG3evFkTJ050bR+ANY7pgV764IMP9O9//7vLvxnqjbvuukvl5eWuPicQbTgSAm7SmTNndP78eT355JOqr6/X6dOnlZiYaD0W0Kfw6TjgJv3mN7/R3LlzdfnyZf3+978nQMBN4EgIAGCGIyEAgBkiBAAwQ4QAAGai7iPa7e3tunDhgpKSkviSRgDogxzHUXNzs9LT0zVgQPfHOlEXoQsXLoS/pgQA0Hc1NDRoxIgR3W4TdafjkpKSrEcAALigJ+/nURchTsEBQGzoyft51EUIANB/ECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAICZiEVo27ZtysrK0qBBgzRp0iQdOnQoUrsCAPRREYnQvn37VFhYqLVr16qmpkYzZszQAw88oPr6+kjsDgDQR3kcx3HcftJ7771XEydO1Pbt28PrRo8erfnz58vv93fYNhQKKRQKhX8OBoPcygEAYkAgEFBycnK327h+JNTS0qLq6mrl5eV1WJ+Xl6cjR4502t7v98vn84UXAgQA/YfrEfrkk0/U1tam1NTUDutTU1PV2NjYafs1a9YoEAiEl4aGBrdHAgBEqYjdWfWr95FwHKfLe0t4vV55vd5IjQEAiGKuHwkNGzZMcXFxnY56mpqaOh0dAQD6N9cjFB8fr0mTJqmioqLD+oqKCk2dOtXt3QEA+rCInI4rKirSY489ptzcXE2ZMkVlZWWqr6/XsmXLIrE7AEAfFZEIPfzww/r000+1fv16/ec//9HYsWP15z//WSNHjozE7gAAfVRE/k6oN4LBoHw+n/UYAIBeMvk7IQAAeooIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGAmYrdyAGJVlH3JiKmubs8CfB0cCQEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABgZqD1AEBPOY5jPQIAl3EkBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADOuR8jv92vy5MlKSkpSSkqK5s+fr5MnT7q9GwBADHA9QpWVlSooKNDRo0dVUVGh1tZW5eXl6cqVK27vCgDQx3mcCN+k5eLFi0pJSVFlZaVmzpx5w+2DwaB8Pl8kR0Ifxf2Eoo/H47EeAVEsEAgoOTm5220iflO7QCAgSRoyZEiXj4dCIYVCofDPwWAw0iMBAKJERD+Y4DiOioqKNH36dI0dO7bLbfx+v3w+X3jJyMiI5EgAgCgS0dNxBQUF+tOf/qTDhw9rxIgRXW7T1ZEQIUJXOB0XfTgdh+6Yno5bsWKFDhw4oIMHD143QJLk9Xrl9XojNQYAIIq5HiHHcbRixQqVl5fr73//u7KystzeBQAgRrgeoYKCAu3du1evv/66kpKS1NjYKEny+XxKSEhwe3cAgD7M9WtC1ztHvGvXLv34xz++4b/nI9q4Hq4JRR+uCaE7JteEeKMAAPQU3x0HADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgJmIR8jv98vj8aiwsDDSuwIA9DERjVBVVZXKyso0bty4SO4GANBHRSxCly9f1qOPPqqdO3fqjjvuiNRuAAB9WMQiVFBQoHnz5un+++/vdrtQKKRgMNhhAQD0DwMj8aSvvPKKjh8/rqqqqhtu6/f79cwzz0RiDABAlHP9SKihoUGrVq3Snj17NGjQoBtuv2bNGgUCgfDS0NDg9kgAgCjlcRzHcfMJ9+/fr4ceekhxcXHhdW1tbfJ4PBowYIBCoVCHx74qGAzK5/O5ORJihMu/qnCBx+OxHgFRLBAIKDk5udttXD8dN2fOHNXV1XVY9/jjjysnJ0fFxcXdBggA0L+4HqGkpCSNHTu2w7rbb79dQ4cO7bQeANC/8Y0JAAAzrl8T6i2uCeF6ouxXFeKaELrXk2tCHAkBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMROR+QkAsi5ZvCeAbJBALOBICAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYGWg9ANDXOI5jPQIQMzgSAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMRCRCH3/8sRYvXqyhQ4cqMTFR99xzj6qrqyOxKwBAH+b6t2hfunRJ06ZN03e/+129+eabSklJ0ZkzZzR48GC3dwUA6ONcj9CmTZuUkZGhXbt2hdfdeeedbu8GABADXD8dd+DAAeXm5mrhwoVKSUnRhAkTtHPnzutuHwqFFAwGOywAgP7B9QidPXtW27dvV3Z2tv7yl79o2bJlWrlypV566aUut/f7/fL5fOElIyPD7ZEAAFHK47h8m8j4+Hjl5ubqyJEj4XUrV65UVVWV3n333U7bh0IhhUKh8M/BYJAQoUvc0TT6eDwe6xEQxQKBgJKTk7vdxvUjoeHDh+vuu+/usG706NGqr6/vcnuv16vk5OQOCwCgf3A9QtOmTdPJkyc7rDt16pRGjhzp9q4AAH2c6xF64okndPToUZWUlOj06dPau3evysrKVFBQ4PauAAB9nOvXhCTpj3/8o9asWaOPPvpIWVlZKioq0k9+8pMe/dtgMCifz+f2SIgBXBOKPlwTQnd6ck0oIhHqDSKE64myX1WICKF7Jh9MAACgp4gQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAw43qEWltb9dRTTykrK0sJCQkaNWqU1q9fr/b2drd3BQDo4wa6/YSbNm3Sjh07tHv3bo0ZM0bvvfeeHn/8cfl8Pq1atcrt3QEA+jDXI/Tuu+/q+9//vubNmydJuvPOO/Xyyy/rvffec3tXAIA+zvXTcdOnT9dbb72lU6dOSZLef/99HT58WA8++GCX24dCIQWDwQ4LAKB/cP1IqLi4WIFAQDk5OYqLi1NbW5s2bNigRYsWdbm93+/XM8884/YYAIA+wPUjoX379mnPnj3au3evjh8/rt27d+t3v/uddu/e3eX2a9asUSAQCC8NDQ1ujwQAiFIex3EcN58wIyNDq1evVkFBQXjdb3/7W+3Zs0f/+te/bvjvg8GgfD6fmyMhRrj8qwoXeDwe6xEQxQKBgJKTk7vdxvUjoc8++0wDBnR82ri4OD6iDQDoxPVrQvn5+dqwYYMyMzM1ZswY1dTUaMuWLVq6dKnbuwIA9HGun45rbm7Wr371K5WXl6upqUnp6elatGiRfv3rXys+Pv6G/57TcbgeTsdFH07HoTs9OR3neoR6iwjheqLsVxUiQuieyTUhAAB6iggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYIYIAQDMECEAgBkiBAAwQ4QAAGaIEADADBECAJghQgAAM0QIAGCGCAEAzBAhAICZgdYDAD3l8XisRwDgMo6EAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADBDhAAAZogQAMAMEQIAmCFCAAAzRAgAYOZrR+jgwYPKz89Xenq6PB6P9u/f3+Fxx3H09NNPKz09XQkJCZo9e7ZOnDjh1rwAgBjytSN05coVjR8/XqWlpV0+vnnzZm3ZskWlpaWqqqpSWlqa5s6dq+bm5l4PCwCIMU4vSHLKy8vDP7e3tztpaWnOxo0bw+uuXr3q+Hw+Z8eOHV0+x9WrV51AIBBeGhoaHEksLCwsLH18CQQCN+yIq9eEzp07p8bGRuXl5YXXeb1ezZo1S0eOHOny3/j9fvl8vvCSkZHh5kgAgCjmaoQaGxslSampqR3Wp6amhh/7qjVr1igQCISXhoYGN0cCAESxiNze+6u3YXYc57q3ZvZ6vfJ6vZEYAwAQ5Vw9EkpLS5OkTkc9TU1NnY6OAABwNUJZWVlKS0tTRUVFeF1LS4sqKys1depUN3cFAIgBX/t03OXLl3X69Onwz+fOnVNtba2GDBmizMxMFRYWqqSkRNnZ2crOzlZJSYkSExP1yCOPuDo4ACAGfN2PZb/zzjtdfhRvyZIl4Y9pr1u3zklLS3O8Xq8zc+ZMp66ursfPHwgEzD9WyMLCwsLS+6UnH9H2OI7jKIoEg0H5fD7rMQAAvRQIBJScnNztNnx3HADADBECAJghQgAAM0QIAGCGCAEAzBAhAIAZIgQAMEOEAABmoi5CUfa3swCAm9ST9/OoixC3AQeA2NCT9/Oo+9qe9vZ2XbhwQUlJSde9B9GNBINBZWRkqKGh4YZfGRHreC064vW4htfiGl6La9x4LRzHUXNzs9LT0zVgQPfHOhG5qV1vDBgwQCNGjHDluZKTk/v9L9SXeC064vW4htfiGl6La3r7WvT0O0Cj7nQcAKD/IEIAADMxGSGv16t169bJ6/Vaj2KO16IjXo9reC2u4bW45la/FlH3wQQAQP8Rk0dCAIC+gQgBAMwQIQCAGSIEADBDhAAAZmIyQtu2bVNWVpYGDRqkSZMm6dChQ9Yj3XJ+v1+TJ09WUlKSUlJSNH/+fJ08edJ6rKjg9/vl8XhUWFhoPYqJjz/+WIsXL9bQoUOVmJioe+65R9XV1dZjmWhtbdVTTz2lrKwsJSQkaNSoUVq/fr3a29utR4u4gwcPKj8/X+np6fJ4PNq/f3+Hxx3H0dNPP6309HQlJCRo9uzZOnHihOtzxFyE9u3bp8LCQq1du1Y1NTWaMWOGHnjgAdXX11uPdktVVlaqoKBAR48eVUVFhVpbW5WXl6crV65Yj2aqqqpKZWVlGjdunPUoJi5duqRp06bptttu05tvvqkPP/xQzz33nAYPHmw9molNmzZpx44dKi0t1T//+U9t3rxZzz77rF544QXr0SLuypUrGj9+vEpLS7t8fPPmzdqyZYtKS0tVVVWltLQ0zZ071/0vmXZizHe+8x1n2bJlHdbl5OQ4q1evNpooOjQ1NTmSnMrKSutRzDQ3NzvZ2dlORUWFM2vWLGfVqlXWI91yxcXFzvTp063HiBrz5s1zli5d2mHdggULnMWLFxtNZEOSU15eHv65vb3dSUtLczZu3Bhed/XqVcfn8zk7duxwdd8xdSTU0tKi6upq5eXldVifl5enI0eOGE0VHQKBgCRpyJAhxpPYKSgo0Lx583T//fdbj2LmwIEDys3N1cKFC5WSkqIJEyZo586d1mOZmT59ut566y2dOnVKkvT+++/r8OHDevDBB40ns3Xu3Dk1NjZ2eC/1er2aNWuW6++lUfct2r3xySefqK2tTampqR3Wp6amqrGx0Wgqe47jqKioSNOnT9fYsWOtxzHxyiuv6Pjx46qqqrIexdTZs2e1fft2FRUV6cknn9SxY8e0cuVKeb1e/ehHP7Ie75YrLi5WIBBQTk6O4uLi1NbWpg0bNmjRokXWo5n68v2yq/fS8+fPu7qvmIrQl756HyLHcW763kSxYPny5frggw90+PBh61FMNDQ0aNWqVfrrX/+qQYMGWY9jqr29Xbm5uSopKZEkTZgwQSdOnND27dv7ZYT27dunPXv2aO/evRozZoxqa2tVWFio9PR0LVmyxHo8c7fivTSmIjRs2DDFxcV1OuppamrqVPT+YsWKFTpw4IAOHjzo2n2a+prq6mo1NTVp0qRJ4XVtbW06ePCgSktLFQqFFBcXZzjhrTN8+HDdfffdHdaNHj1af/jDH4wmsvWLX/xCq1ev1g9/+ENJ0re//W2dP39efr+/X0coLS1N0v+PiIYPHx5eH4n30pi6JhQfH69JkyapoqKiw/qKigpNnTrVaCobjuNo+fLleu211/T2228rKyvLeiQzc+bMUV1dnWpra8NLbm6uHn30UdXW1vabAEnStGnTOn1U/9SpUxo5cqTRRLY+++yzTnf+jIuL6xcf0e5OVlaW0tLSOryXtrS0qLKy0vX30pg6EpKkoqIiPfbYY8rNzdWUKVNUVlam+vp6LVu2zHq0W6qgoEB79+7V66+/rqSkpPDRoc/nU0JCgvF0t1ZSUlKna2G33367hg4d2u+ukT3xxBOaOnWqSkpK9IMf/EDHjh1TWVmZysrKrEczkZ+frw0bNigzM1NjxoxRTU2NtmzZoqVLl1qPFnGXL1/W6dOnwz+fO3dOtbW1GjJkiDIzM1VYWKiSkhJlZ2crOztbJSUlSkxM1COPPOLuIK5+1i5KbN261Rk5cqQTHx/vTJw4sV9+LFlSl8uuXbusR4sK/fUj2o7jOG+88YYzduxYx+v1Ojk5OU5ZWZn1SGaCwaCzatUqJzMz0xk0aJAzatQoZ+3atU4oFLIeLeLeeeedLt8jlixZ4jjO/z+mvW7dOictLc3xer3OzJkznbq6Otfn4H5CAAAzMXVNCADQtxAhAIAZIgQAMEOEAABmiBAAwAwRAgCYIUIAADNECABghggBAMwQIQCAGSIEADDzP4ywxILHg4zzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "### Build a Convolutional Neral Network Class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "        \n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "<h2 align=center>Analyse Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:  \n",
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. \n",
    "\n",
    "Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork952-2022-01-01) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
