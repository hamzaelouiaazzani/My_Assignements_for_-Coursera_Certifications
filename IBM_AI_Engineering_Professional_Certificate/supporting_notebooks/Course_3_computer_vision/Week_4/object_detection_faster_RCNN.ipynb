{"cells":[{"cell_type":"markdown","id":"238e1f95-c0fb-4caa-86b0-cbb70a5661d9","metadata":{},"outputs":[],"source":["\u003cimg src=\"http://vision.skills.network/logo-light.png\" width=\"400\" alt=\"CV Studio logo\"\u003e\n"]},{"cell_type":"markdown","id":"85678063-489b-49f7-8167-b4558f36a86a","metadata":{},"outputs":[],"source":["**\u003ch1\u003e Object detection with Faster R-CNN \u003c/h1\u003e**\n"]},{"cell_type":"markdown","id":"28645b4c-99b6-4085-b799-d428a568aa38","metadata":{},"outputs":[],"source":["Faster R-CNN is a method for object detection that uses region proposal.  In this lab, you will use Faster R-CNN pre-trained on the coco dataset. You will learn how to detect several   objects by name and to use the likelihood of the object prediction being correct.\n"]},{"cell_type":"markdown","id":"d39f23de-9f79-43d5-92ec-67c5c9c823c6","metadata":{},"outputs":[],"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"e97e2cab-8c29-429f-9a61-f01c9f4aa5fa","metadata":{},"outputs":[],"source":["\u003ch2\u003eObjectives\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"574461ad-b880-4b7c-9e97-f8bb16e47959","metadata":{},"outputs":[],"source":["Apply Object detection with Faster R-CNN to classify  predetermined objects using objects name and/or to use the likelihood of the object. \n"]},{"cell_type":"markdown","id":"e9fc066f-5b6f-4ab0-b5af-f7324c136489","metadata":{},"outputs":[],"source":["\u003cul\u003e\n","    \u003cli\u003e\u003ca href='#MI'\u003eObject detection with Faster R-CNN \u003c/a\u003e\n","        \u003cul\u003e\n","            \u003cli\u003eImport Libraries and Define Auxiliary Functions  \u003c/li\u003e\n","            \u003cli\u003eLoad Pre-trained Faster R-CNN \u003c/li\u003e\n","            \u003cli\u003eObject Localization \u003c/li\u003e\n","            \u003cli\u003eObject Detection  \u003c/li\u003e\n","            \u003cli\u003eTest Model With An Uploaded Image \u003c/li\u003e\n","     \n","  \n","    \n","\u003c/ul\u003e\n"]},{"cell_type":"markdown","id":"a93f273d-f63c-48a7-b26e-ba40967a004a","metadata":{},"outputs":[],"source":["----\n"]},{"cell_type":"markdown","id":"c34d9978-bd41-46da-b982-73e1c2f23c4e","metadata":{},"outputs":[],"source":[" Download the image for the labs:\n"]},{"cell_type":"code","id":"2a935328-e667-4487-aac4-1c19836a6181","metadata":{},"outputs":[],"source":["! pip3 install torch==1.13.0 torchvision==0.14.0 torchaudio"]},{"cell_type":"code","id":"c70f5e76-8fee-46b1-aeb1-410e4df99902","metadata":{},"outputs":[],"source":["! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/images%20/images_part_5/DLguys.jpeg\n! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/images%20/images_part_5/watts_photos2758112663727581126637_b5d4d192d4_b.jpeg\n! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/images%20/images_part_5/istockphoto-187786732-612x612.jpeg\n! wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/images%20/images_part_5/jeff_hinton.png"]},{"cell_type":"markdown","id":"1bcb4b18-7d5d-485d-b0b6-ab5b43a6e1bc","metadata":{},"outputs":[],"source":["\n","## Import Libraries and Define Auxiliary Functions\n"]},{"cell_type":"markdown","id":"091b1ff4-ed40-4dd5-8b04-63ca67a2ed02","metadata":{},"outputs":[],"source":["deep-learning libraries , may have to update:\n"]},{"cell_type":"code","id":"6780f00c-92fd-4568-86d5-8685de7a5281","metadata":{},"outputs":[],"source":["#! conda install pytorch=1.1.0 torchvision -c pytorch -y"]},{"cell_type":"code","id":"666af2ec-c75e-464d-99c2-14cca3ddaf4c","metadata":{},"outputs":[],"source":["import torchvision\nfrom torchvision import  transforms \nimport torch\nfrom torch import no_grad"]},{"cell_type":"markdown","id":"3d2c3076-8320-4868-8fb1-567962979cb2","metadata":{},"outputs":[],"source":["libraries for getting data from the web  \n"]},{"cell_type":"code","id":"70a9318b-05e9-4ccf-a634-75056a279362","metadata":{},"outputs":[],"source":["import requests"]},{"cell_type":"markdown","id":"1508e0d9-9550-4c38-8532-a5ae67e86d2c","metadata":{},"outputs":[],"source":["libraries  for image processing and visualization\n"]},{"cell_type":"code","id":"4bbf0852-9771-4e36-9cb8-70a79565a9e9","metadata":{},"outputs":[],"source":["import cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"2defca6d-725e-478a-9488-cf030cee0608","metadata":{},"outputs":[],"source":["This function will assign a string name to a predicted class and eliminate predictions whose likelihood  is under a threshold.\n"]},{"cell_type":"code","id":"a9f8facb-e9b1-4011-ab1b-a995cf33d665","metadata":{},"outputs":[],"source":["def get_predictions(pred,threshold=0.8,objects=None ):\n    \"\"\"\n    This function will assign a string name to a predicted class and eliminate predictions whose likelihood  is under a threshold \n    \n    pred: a list where each element contains a tuple that corresponds to information about  the different objects; Each element includes a tuple with the class yhat, probability of belonging to that class and the coordinates of the bounding box corresponding to the object \n    image : frozen surface\n    predicted_classes: a list where each element contains a tuple that corresponds to information about  the different objects; Each element includes a tuple with the class name, probability of belonging to that class and the coordinates of the bounding box corresponding to the object \n    thre\n    \"\"\"\n\n\n    predicted_classes= [(COCO_INSTANCE_CATEGORY_NAMES[i],p,[(box[0], box[1]), (box[2], box[3])]) for i,p,box in zip(list(pred[0]['labels'].numpy()),pred[0]['scores'].detach().numpy(),list(pred[0]['boxes'].detach().numpy()))]\n    predicted_classes=[  stuff  for stuff in predicted_classes  if stuff[1]\u003ethreshold ]\n    \n    if objects  and predicted_classes :\n        predicted_classes=[ (name, p, box) for name, p, box in predicted_classes if name in  objects ]\n    return predicted_classes"]},{"cell_type":"markdown","id":"9a5c4813-7b95-499c-94b0-2961c5128f61","metadata":{},"outputs":[],"source":["Draws box around each object\n"]},{"cell_type":"code","id":"c5b4774b-d9d2-4554-b4b1-396763ad49ea","metadata":{},"outputs":[],"source":["def draw_box(predicted_classes,image,rect_th= 10,text_size= 3,text_th=3):\n    \"\"\"\n    draws box around each object \n    \n    predicted_classes: a list where each element contains a tuple that corresponds to information about  the different objects; Each element includes a tuple with the class name, probability of belonging to that class and the coordinates of the bounding box corresponding to the object \n    image : frozen surface \n   \n    \"\"\"\n\n    img=(np.clip(cv2.cvtColor(np.clip(image.numpy().transpose((1, 2, 0)),0,1), cv2.COLOR_RGB2BGR),0,1)*255).astype(np.uint8).copy()\n    for predicted_class in predicted_classes:\n   \n        label=predicted_class[0]\n        probability=predicted_class[1]\n        box=predicted_class[2]\n\n        cv2.rectangle(img, box[0], box[1],(0, 255, 0), rect_th) # Draw Rectangle with the coordinates\n        cv2.putText(img,label, box[0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) \n        cv2.putText(img,label+\": \"+str(round(probability,2)), box[0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    del(img)\n    del(image)"]},{"cell_type":"markdown","id":"cce8860a-bfb8-4782-aef6-58f68c7c41df","metadata":{},"outputs":[],"source":["this function  will speed up your code by freeing memory.\n"]},{"cell_type":"markdown","id":"604c3523-d329-4878-abad-4d4d013efc3e","metadata":{},"outputs":[],"source":["this function will free up some memory:\n"]},{"cell_type":"code","id":"80483ae6-14e3-48eb-b098-89444ed64261","metadata":{},"outputs":[],"source":["def save_RAM(image_=False):\n    global image, img, pred\n    torch.cuda.empty_cache()\n    del(img)\n    del(pred)\n    if image_:\n        image.close()\n        del(image)"]},{"cell_type":"markdown","id":"29cf2512-75b9-427f-b512-2995753edf3d","metadata":{},"outputs":[],"source":["## Load Pre-trained Faster R-CNN\n"]},{"cell_type":"markdown","id":"9225db84-d3d8-43fb-982b-14732caa8c78","metadata":{},"outputs":[],"source":["\u003ca href='https://arxiv.org/abs/1506.01497?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01'\u003eFaster R-CNN\u003c/a\u003e is a model that predicts both bounding boxes and class scores for potential objects in the image  pre-trained on \u003ca href=\"https://cocodataset.org/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\"\u003eCOCO\u003ca\u003e. \n"]},{"cell_type":"code","id":"478ca26c-d7e0-4f21-9a25-5bdd3397cf58","metadata":{},"outputs":[],"source":["model_ = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nmodel_.eval()\n\nfor name, param in model_.named_parameters():\n    param.requires_grad = False\nprint(\"done\")"]},{"cell_type":"markdown","id":"c4ae01d7-662a-4bd4-bf54-080f880c31c7","metadata":{},"outputs":[],"source":["the function calls Faster R-CNN \u003ccode\u003e model_ \u003c/code\u003e but save RAM:\n"]},{"cell_type":"code","id":"0e73dce3-d801-41a6-b332-d62c7f99dfe7","metadata":{},"outputs":[],"source":["def model(x):\n    with torch.no_grad():\n        yhat = model_(x)\n    return yhat"]},{"cell_type":"markdown","id":"1bec568c-c186-4db2-8d1a-2f051962897f","metadata":{},"outputs":[],"source":["Here are the 91 classes.\n"]},{"cell_type":"code","id":"4e2b39c4-4688-4b4a-ae2b-4fb9f7996c83","metadata":{},"outputs":[],"source":["COCO_INSTANCE_CATEGORY_NAMES = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\nlen(COCO_INSTANCE_CATEGORY_NAMES)"]},{"cell_type":"markdown","id":"be8e8c25-5b70-4eff-bd5b-b6eafe15bf77","metadata":{},"outputs":[],"source":["## Object Localization\n"]},{"cell_type":"markdown","id":"c059dc2b-3f20-41e5-96a9-98157e23fa47","metadata":{},"outputs":[],"source":["In Object Localization we locate the presence of objects in an image and indicate the location with a bounding box. Consider the image of \u003ca href=\"https://www.utoronto.ca/news/ai-fuels-boom-innovation-investment-and-jobs-canada-report-says?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\"\u003e Geoffrey Hinton\u003c/a\u003e\n"]},{"cell_type":"code","id":"abd56fcb-da17-42e0-9cfa-9982cef961f1","metadata":{},"outputs":[],"source":["img_path='jeff_hinton.png'\nhalf = 0.5\nimage = Image.open(img_path)\n\nimage.resize( [int(half * s) for s in image.size] )\n\nplt.imshow(image)\nplt.show()"]},{"cell_type":"markdown","id":"59dd0f14-36b1-455a-b7fe-f66a464a32f5","metadata":{},"outputs":[],"source":["We will create a transform object to convert the image to a tensor.\n"]},{"cell_type":"code","id":"2db8544b-fd9e-403a-b5b0-ef2c65719a3c","metadata":{},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor()])"]},{"cell_type":"markdown","id":"28856d4f-3333-42f0-ae9f-eda9fe568e0e","metadata":{},"outputs":[],"source":["We convert the image to a tensor.\n"]},{"cell_type":"code","id":"a98aecea-dd1f-4338-9167-a1158eca4521","metadata":{},"outputs":[],"source":["img = transform(image)"]},{"cell_type":"markdown","id":"d32240ea-ce3b-45f0-b2c5-92be0a0ac8f7","metadata":{},"outputs":[],"source":["we can make a prediction,The output is a dictionary with several predicted classes, the probability of belonging to that class and the coordinates of the bounding box corresponding to that class.\n"]},{"cell_type":"code","id":"60cfaa1c-3af6-4010-9160-b234c03b543e","metadata":{},"outputs":[],"source":["pred = model([img])"]},{"cell_type":"markdown","id":"7fae0b3a-6e17-4980-836a-62daeee3dd67","metadata":{},"outputs":[],"source":["\u003cb\u003enote\u003c/b\u003e:  if you call \u003ccode\u003emodel_([img])\u003c/code\u003e  directly but it will use more RAM \n"]},{"cell_type":"markdown","id":"703e7eac-ae93-4d4e-b50f-5f84e7478f21","metadata":{},"outputs":[],"source":["we have the 35  different class predictions, ordered by likelihood scores for potential objects.\n"]},{"cell_type":"code","id":"40009eb3-94f2-4bdf-8590-683ff2d867d1","metadata":{},"outputs":[],"source":["pred[0]['labels']"]},{"cell_type":"markdown","id":"8bbed256-0589-4e88-9ad6-6c6c4f79c3e1","metadata":{},"outputs":[],"source":["We have the likelihood of each class:\n"]},{"cell_type":"code","id":"c7875213-9b49-4d78-a179-95478b1c1f96","metadata":{},"outputs":[],"source":["pred[0]['scores']"]},{"cell_type":"markdown","id":"783d8e50-9749-4d83-a7f1-68e971887dbc","metadata":{},"outputs":[],"source":["*Note* here we use likelihood as a synonym for probability. Many neural networks output a probability of the output of being a specific class. Here the output is the confidence of  prediction, so we use the term likelihood to distinguish between the two \n"]},{"cell_type":"markdown","id":"cabdfb97-6519-43d6-aa37-4857aeca906f","metadata":{},"outputs":[],"source":["The class number corresponds to the index of the list with the corresponding  category name \n"]},{"cell_type":"code","id":"4b6156bd-e5e0-4bbc-83af-e53937d0553c","metadata":{},"outputs":[],"source":["index=pred[0]['labels'][0].item()\nCOCO_INSTANCE_CATEGORY_NAMES[index]"]},{"cell_type":"markdown","id":"469e3cde-1793-4614-8fa1-c6b4a63b75be","metadata":{},"outputs":[],"source":["we have the coordinates of the bounding box\n"]},{"cell_type":"code","id":"d3bb2892-477f-43ce-949a-e260f1ecde77","metadata":{},"outputs":[],"source":["bounding_box=pred[0]['boxes'][0].tolist()\nbounding_box"]},{"cell_type":"markdown","id":"9c959226-8b87-45f7-8c4e-c0f80ab73f21","metadata":{},"outputs":[],"source":["These components correspond to the top-left corner and bottom-right corner of the rectangle,more precisely :\n","\u003cp\u003etop (t),left (l),bottom(b),right (r)\u003c/p\u003e\n","we need to round them\n"]},{"cell_type":"code","id":"e62dd46a-2691-42bb-a93e-e3bd91abe265","metadata":{},"outputs":[],"source":["t,l,r,b=[round(x) for x in bounding_box]"]},{"cell_type":"markdown","id":"4d279de1-f4a0-4be6-99f4-8b21d70ac0ac","metadata":{},"outputs":[],"source":["We convert the tensor to an open CV array and plot an image with the box:\n"]},{"cell_type":"code","id":"6ca221b0-5ec0-411c-89ff-e5fb6ae3d139","metadata":{},"outputs":[],"source":["img_plot=(np.clip(cv2.cvtColor(np.clip(img.numpy().transpose((1, 2, 0)),0,1), cv2.COLOR_RGB2BGR),0,1)*255).astype(np.uint8)\ncv2.rectangle(img_plot,(t,l),(r,b),(0, 255, 0), 10) # Draw Rectangle with the coordinates\nplt.imshow(cv2.cvtColor(img_plot, cv2.COLOR_BGR2RGB))\nplt.show()\ndel img_plot, t, l, r, b"]},{"cell_type":"markdown","id":"546a902c-0f9a-443c-a862-2cd2b07aec7c","metadata":{},"outputs":[],"source":["We can localize objects; we do this using the function \n","\u003ccode\u003eget_predictions\u003c/code\u003e. The input  is the predictions \u003ccode\u003epred\u003c/code\u003e and the \u003ccode\u003eobjects\u003c/code\u003e you would like to localize .\n"]},{"cell_type":"code","id":"c75b1fb5-03d2-4417-b99d-279693453c7f","metadata":{},"outputs":[],"source":["pred_class=get_predictions(pred,objects=\"person\")\ndraw_box(pred_class, img)\ndel pred_class"]},{"cell_type":"markdown","id":"57011146-3eba-43bc-add8-00c0d9401729","metadata":{},"outputs":[],"source":["We can set a threshold \u003ccode\u003ethreshold \u003c/code\u003e. Here we set the  threshold 1 i.e Here we set the  threshold 1 i.e. 100% likelihood. \n"]},{"cell_type":"code","id":"3f53b98d-ca2b-425a-b541-664b2bbf8440","metadata":{},"outputs":[],"source":["get_predictions(pred,threshold=1,objects=\"person\")"]},{"cell_type":"markdown","id":"b678d7bd-aaa3-489b-9c48-db39963c6b58","metadata":{},"outputs":[],"source":["Here we have no output as the likelihood is not 100%.  Let's try a threshold of 0.98 and use the function  draw_box to draw the box and plot the class and it's rounded likelihood.\n"]},{"cell_type":"code","id":"d6e782c4-8f87-4cb5-be60-981a63479b84","metadata":{},"outputs":[],"source":["pred_thresh=get_predictions(pred,threshold=0.98,objects=\"person\")\ndraw_box(pred_thresh,img)\ndel pred_thresh"]},{"cell_type":"markdown","id":"3b5575bf-5204-402f-9f1a-bb4b0588ade5","metadata":{},"outputs":[],"source":["Delete objects to save memory, we will run this after every cell:\n"]},{"cell_type":"code","id":"bd6d6ee8-1a9f-4df7-a821-8a56f710c02a","metadata":{},"outputs":[],"source":["save_RAM(image_=True)"]},{"cell_type":"markdown","id":"765446ad-d5e8-46fe-9f24-ff0b587058f8","metadata":{},"outputs":[],"source":["We can locate multiple objects, consider the following \u003ca href='https://www.kdnuggets.com/2015/03/talking-machine-deep-learning-gurus-p1.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01'\u003eimage\u003c/a\u003e, we can detect the people in the image.\n"]},{"cell_type":"code","id":"999d8867-a429-4ba8-8c44-9d658602d5ad","metadata":{},"outputs":[],"source":["img_path='DLguys.jpeg'\nimage = Image.open(img_path)\nimage.resize([int(half * s) for s in image.size])\nplt.imshow(np.array(image))\nplt.show()"]},{"cell_type":"markdown","id":"b6548862-5b23-4e3f-858a-f321766fc693","metadata":{},"outputs":[],"source":["we can set a threshold to detect the object, 0.9 seems to work.\n"]},{"cell_type":"code","id":"00ff2774-c1ec-40b8-a0b7-f3de7a2bed0d","metadata":{},"outputs":[],"source":["img = transform(image)\npred = model([img])\npred_thresh=get_predictions(pred,threshold=0.8,)\ndraw_box(pred_thresh,img,rect_th= 1,text_size= 0.5,text_th=1)\ndel pred_thresh"]},{"cell_type":"markdown","id":"983164ea-234c-44da-9f08-894c46e4a620","metadata":{},"outputs":[],"source":["Or we can use objects parameter: \n"]},{"cell_type":"code","id":"7a0e538f-2057-4802-b343-593564aac5c9","metadata":{},"outputs":[],"source":["\npred_obj=get_predictions(pred,objects=\"person\")\ndraw_box(pred_obj,img,rect_th= 1,text_size= 0.5,text_th=1)\ndel pred_obj"]},{"cell_type":"markdown","id":"4ea87654-ab72-48b8-b946-52ca18f92206","metadata":{},"outputs":[],"source":["If we set the threshold too low, we will detect objects that are not there.\n"]},{"cell_type":"code","id":"03ceb084-0d1c-42a5-80ea-b9b88a076337","metadata":{},"outputs":[],"source":["pred_thresh=get_predictions(pred,threshold=0.01)\ndraw_box(pred_thresh,img,rect_th= 1,text_size= 0.5,text_th=1)\ndel pred_thresh"]},{"cell_type":"markdown","id":"614ffd3a-c0b0-4f60-9bda-b85a448e08b1","metadata":{},"outputs":[],"source":["the following lines will speed up your code by using less RAM.\n"]},{"cell_type":"code","id":"9991ef74-f8a2-4175-bb10-38999fde0601","metadata":{},"outputs":[],"source":["save_RAM(image_=True)"]},{"cell_type":"markdown","id":"0b97ea3c-97cd-4902-bb38-3643c5eb2352","metadata":{},"outputs":[],"source":["## Object Detection \n"]},{"cell_type":"markdown","id":"603330c7-1047-488d-8652-3316bd8fb690","metadata":{},"outputs":[],"source":["In Object Detection we find the classes as well detect the objects in an image. Consider the following \u003ca href=\"https://www.dreamstime.com/stock-image-golden-retriever-puppy-lying-parakeet-perched-its-head-weeks-old-next-to-british-shorthair-kitten-sitting-image30336051?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01\"\u003eimage\u003c/a\u003e\n"]},{"cell_type":"code","id":"eb78ab72-6d87-42e3-8060-891032d35296","metadata":{},"outputs":[],"source":["img_path='istockphoto-187786732-612x612.jpeg'\nimage = Image.open(img_path)\nimage.resize( [int(half * s) for s in image.size] )\nplt.imshow(np.array(image))\nplt.show()\ndel img_path"]},{"cell_type":"markdown","id":"c6533813-9115-4666-9d59-852f8bb233c0","metadata":{},"outputs":[],"source":["If we set a threshold, we can detect all objects whose likelihood is above that threshold.\n"]},{"cell_type":"code","id":"bf664957-4f0c-4eab-9f5e-9e9ca5833226","metadata":{},"outputs":[],"source":["img = transform(image)\npred = model([img])\npred_thresh=get_predictions(pred,threshold=0.97)\ndraw_box(pred_thresh,img,rect_th= 1,text_size= 1,text_th=1)\ndel pred_thresh"]},{"cell_type":"markdown","id":"8f046ed1-472f-464e-ae1c-f4c1fe210bbc","metadata":{},"outputs":[],"source":["the following lines will speed up your code by using less RAM.\n"]},{"cell_type":"code","id":"368d9656-dc78-4891-9d9b-828b1318d64d","metadata":{},"outputs":[],"source":[" save_RAM(image_=True)"]},{"cell_type":"markdown","id":"68c27b92-72f6-40e8-815b-79f8c32aeb8b","metadata":{},"outputs":[],"source":["We can specify the objects we would like to classify, for example, cats and dogs:\n"]},{"cell_type":"code","id":"82bbe252-5580-4107-90f4-3f7dce5d77c6","metadata":{},"outputs":[],"source":["img_path='istockphoto-187786732-612x612.jpeg'\nimage = Image.open(img_path)\nimg = transform(image)\npred = model([img])\npred_obj=get_predictions(pred,objects=[\"dog\",\"cat\"])\ndraw_box(pred_obj,img,rect_th= 1,text_size= 0.5,text_th=1)\ndel pred_obj\n\n"]},{"cell_type":"code","id":"b4d327ea-5d7a-46ef-836d-ab2b57f36369","metadata":{},"outputs":[],"source":["# save_RAM()"]},{"cell_type":"markdown","id":"81e1fa29-aa0e-41d3-9fa5-560adf206bd6","metadata":{},"outputs":[],"source":["If we set the threshold too low, we may detect objects with a low likelihood of being correct; here, we set the threshold to 0.7, and we incorrectly  detect a cat \n"]},{"cell_type":"code","id":"620698d9-98c9-47e0-b898-91ff6b6534c7","metadata":{},"outputs":[],"source":["# img = transform(image)\n# pred = model([img])\npred_thresh=get_predictions(pred,threshold=0.70,objects=[\"dog\",\"cat\"])\ndraw_box(pred_thresh,img,rect_th= 1,text_size= 1,text_th=1)\ndel pred_thresh"]},{"cell_type":"code","id":"13da28c4-b694-4da0-9621-cb8fc82a029a","metadata":{},"outputs":[],"source":["save_RAM(image_=True)"]},{"cell_type":"markdown","id":"20f9aa0e-4d43-4b25-9245-c74e52010017","metadata":{},"outputs":[],"source":["\n","We can detect other objects. Consider the following \u003ca href='https://www.flickr.com/photos/watts_photos/27581126637?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01'\u003eimage\u003c/a\u003e; We can detect cars and airplanes \n"]},{"cell_type":"code","id":"2c84450e-8575-4573-91f0-b06e9c6ee897","metadata":{},"outputs":[],"source":["img_path='watts_photos2758112663727581126637_b5d4d192d4_b.jpeg'\nimage = Image.open(img_path)\nimage.resize( [int(half * s) for s in image.size] )\nplt.imshow(np.array(image))\nplt.show()\ndel img_path"]},{"cell_type":"code","id":"d0ace912-2c84-4f9d-8c5b-b47212832d34","metadata":{},"outputs":[],"source":["img = transform(image)\npred = model([img])\npred_thresh=get_predictions(pred,threshold=0.997)\ndraw_box(pred_thresh,img)\ndel pred_thresh"]},{"cell_type":"code","id":"4f5eead0-3a1c-4890-a8d1-3d134c6d9722","metadata":{},"outputs":[],"source":["save_RAM(image_=True)"]},{"cell_type":"markdown","id":"d553a858-1442-478e-9204-1df211c2def3","metadata":{},"outputs":[],"source":["## Test Model With An Uploaded Image\n"]},{"cell_type":"markdown","id":"1487f784-3ef8-4966-8fe0-d4c4888bcacd","metadata":{},"outputs":[],"source":["You can enter the URL of an image and see if you can detect objects in it . Just remember it must have an image  extension like \u003ccode\u003ejpg\u003c/code\u003e or \u003ccode\u003epng\u003c/code\u003e.\n"]},{"cell_type":"code","id":"61d4e3ec-95df-4f09-9899-4396626a2295","metadata":{},"outputs":[],"source":["url='https://www.plastform.ca/wp-content/themes/plastform/images/slider-image-2.jpg'"]},{"cell_type":"markdown","id":"ab7c9ecb-0924-434e-b0ff-e4692190ca7a","metadata":{},"outputs":[],"source":["We will perform a get request to download the image from the web and convert it to an RGB image. \n"]},{"cell_type":"code","id":"58c0a458-4330-4869-9bb7-cdc2e34c0484","metadata":{},"outputs":[],"source":["image = Image.open(requests.get(url, stream=True).raw).convert('RGB')\ndel url"]},{"cell_type":"code","id":"9f24765c-5e3b-4c31-ba94-aa1396907ed2","metadata":{},"outputs":[],"source":["img = transform(image )\npred = model([img])\npred_thresh=get_predictions(pred,threshold=0.95)\ndraw_box(pred_thresh, img)\ndel pred_thresh"]},{"cell_type":"code","id":"ba98077d-94b8-4a35-8819-fbf116b69e9d","metadata":{},"outputs":[],"source":["save_RAM(image_=True)"]},{"cell_type":"markdown","id":"ccf50457-a19a-43d0-a20a-67fb5402184f","metadata":{},"outputs":[],"source":["Upload your image, and see if you can detect an object \n","\u003cp\u003e\u003cb\u003eInstructions on how to upload image:\u003c/b\u003e\u003c/p\u003e\n","Use the upload button and upload the image from your local machine\n","\u003ccenter\u003e\n","    \u003cimg src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-SkillsNetwork/images/instruction.png\" width=\"300\"\u003e\n","\u003c/center\u003e\n"]},{"cell_type":"markdown","id":"4f48087f-84bf-4053-9b61-bb3e52aa930a","metadata":{},"outputs":[],"source":["Replace with the name of your image as seen in your directory\n"]},{"cell_type":"code","id":"83a2485b-2d83-45d5-aede-67580866481e","metadata":{},"outputs":[],"source":["# img_path='Replace with the name of your image as seen in your directory'\n# image = Image.open(img_path) # Load the image\n# plt.imshow(np.array(image ))\n# plt.show()"]},{"cell_type":"markdown","id":"e85f2824-0204-4795-b7b8-dabe902485cc","metadata":{},"outputs":[],"source":["detect objects \n"]},{"cell_type":"code","id":"3b3d9793-d42c-4ebb-a50e-59200d690cdd","metadata":{},"outputs":[],"source":["# img = transform(image )\n# pred = model(img.unsqueeze(0))\n# pred_thresh=get_predictions(pred,threshold=0.95)\n# draw_box(pred_thresh,img)"]},{"cell_type":"markdown","id":"0a86afe1-e7d5-446e-95b4-8b32b022c487","metadata":{},"outputs":[],"source":["\u003ch2\u003eAuthors\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"e0ab2715-fd93-4375-a3d5-1d80b914c8d6","metadata":{},"outputs":[],"source":[" [Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"cf462c27-0a47-42af-913b-2bc316112e1f","metadata":{},"outputs":[],"source":["\u003ch3\u003eOther Contributors\u003c/h3\u003e\n"]},{"cell_type":"markdown","id":"c00ef9b1-95d5-476d-9aca-6c823dbb7731","metadata":{},"outputs":[],"source":["\u003ca href=\"contributor_link\"\u003eContributor with Link\u003c/a\u003e, Contributor No Link\n"]},{"cell_type":"markdown","id":"984ebe31-6a1b-46d0-8170-2f74a0a48cd4","metadata":{},"outputs":[],"source":["# References \n"]},{"cell_type":"markdown","id":"21497999-e055-47f5-b97d-91669080828d","metadata":{},"outputs":[],"source":["[1]  Images were taken from: https://homepages.cae.wisc.edu/~ece533/images/\n","    \n","[2]  \u003ca href='https://pillow.readthedocs.io/en/stable/index.html?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01'\u003ePillow Docs\u003c/a\u003e\n","\n","[3]  \u003ca href='https://opencv.org/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkCV0101ENCoursera872-2023-01-01'\u003eOpen CV\u003c/a\u003e\n","\n","[4] Gonzalez, Rafael C., and Richard E. Woods. \"Digital image processing.\" (2017).\n"]},{"cell_type":"markdown","id":"10928ca7-e3c4-4c4d-8133-5aa8e91e4c9f","metadata":{},"outputs":[],"source":["\u003ch2\u003eChange Log\u003c/h2\u003e\n"]},{"cell_type":"markdown","id":"010aa144-d30b-49cc-8a48-3de8a3cd3ecd","metadata":{},"outputs":[],"source":["\u003ctable\u003e\n","    \u003ctr\u003e\n","        \u003cth\u003eDate (YYYY-MM-DD)\u003c/th\u003e\n","        \u003cth\u003eVersion\u003c/th\u003e\n","        \u003cth\u003eChanged By\u003c/th\u003e\n","        \u003cth\u003eChange Description\u003c/th\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","        \u003ctd\u003e2020-07-20\u003c/td\u003e\n","        \u003ctd\u003e0.2\u003c/td\u003e\n","        \u003ctd\u003eJoseph Santarcangelo \u003c/td\u003e\n","        \u003ctd\u003eModified Multiple Areas\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","        \u003ctd\u003e2020-07-17\u003c/td\u003e\n","        \u003ctd\u003e0.1\u003c/td\u003e\n","        \u003ctd\u003eAzim\u003c/td\u003e\n","        \u003ctd\u003eCreated Lab Template\u003c/td\u003e\n","    \u003c/tr\u003e\n","\u003c/table\u003e\n"]},{"cell_type":"markdown","id":"da902029-119b-4d4d-af1f-3c6902ae2743","metadata":{},"outputs":[],"source":["Copyright Â© 2020 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":4}